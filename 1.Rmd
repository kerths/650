---
title: "Group project"
author: "Zongrui Dai"
date: "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
---

## Input the data and do imputation
# Data input
```{r}
library(xlsx)
# library(readxl)
library(ggplot2)
library(ggstatsplot)
library(nnet)
library(MASS)
library(lmtest)
library(sandwich)
library(dplyr)
setwd('D:/BIOSTAT 650/Group project/')
```

## Input data
```{r}
data<-read.xlsx('Depression Data.xls',header=T,sheetName = 'dep')
# data <- read_excel("Depression Data.xls")
str(data)
```

```{r}
df1<-data.frame(
  Grade = data$GRADE,
  USborn = data$USBORN,
  Age = data$AGE4,
  Marstat = data$MARSTAT4,
  Khyper41 = data$KHYPER41,
  Mdiab41 = data$MDIAB41,
  Totmmse4 = data$TOTMMSE4,
  Cesdtot4 = data$CESDTOT4,
  Totiadl4 = data$TOTIADL4,
  EE46 = data$EE46,
  OO49lang = data$OO49LANG,
  Sex = data$MALE,
  U43s = data$U43S,
  Totadl4 = data$TOTADL4,
  NKIDS4 = data$NKIDS4,
  HEALTH4 = data$HEALTH4,
  NFRAC41 = data$NFRAC41,
  CC43 = data$CC43,
  HHA4 = data$HHA4
)

#l1<-lm(Cesdtot4~.,na.omit(df1))
#l2<-lm(Cesdtot4~.,na.omit(df))
#anova(l1,l2)

NA_Count<-c()
for(i in length(df1[,1])){
  NA_Count<-c(NA_Count,sum(is.na(df1[i,])))
}
colSums(is.na(df1)) # CC43 and HHA4 have a lot of missingness
```

### Variables we are interested in analyzing

USborn- literature suggests relationship between immigration status and depression (Black, 1998)
Age - literature suggests relationship between age and depression (Gonzalez, 2001)
Marital status - literature (Gonzalez, 2001)
**not nkids (no lit evidence)
Selfrated health (health4) - relationship between health locus of control, number of chronic conditions and depression (Black, 1998)... self-rated health could be related to these things and so might be an important predictor 
Khyper - literature (reference?)
Mdiab41 - literature (reference?)
** not nfrac41- mixed literature evidence (references?) 
u43s (high cholesterol)- literature (reference?)
totmmse4 (cognitive impairment) - relationship between disability and depression (Black, 1998) but there might be a better (more relevant) reference
totiadl4 - predictor of interest
totdl4 - related to predictor of interest
** not cc43 (life satisfaction) - seems too similar to depression
ee46 (religiosity) - predictor of interest
** not hha4 (bedridden) - already captured by totdl4 (See plots below)
oo49lang - literature (reference?) 
sex - literature (Black, 1998; Gonzalez, 2001)

The high missingness of cc43 and hha4 is another reason to exclude these variables altogether 

```{r relationship bewteen HHA4 and Totadl4}
ggplot(data=na.omit(df1), aes(group=HHA4, y=Totadl4)) +
  geom_boxplot()
hist(df1$Totadl4[df1$HHA4==0], main='Not bedridden', xlab='Totdl4')
hist(df1$Totadl4[df1$HHA4==1], main='Bedridden', xlab='Totdl4')
# one of these variables has to be wrong. You can't be bedridden and walking/getting up from a chair
hist(df1$Totadl4[is.na(df1$HHA4)], main='Missing')
```


###############################################################################################
#                                   Imputation                                                #
###############################################################################################

# Do we include all variables in imputation, or just the ones we plan to include in our analysis? I think we should only include the ones we plan to include in downstream analysis...

## Review the pattern of missing value
# First figure indicate which type of sample are likely to be missed
# Second figure indicate that Cesdtot4,Totmmse4, and EE46 have higher propotion of missing value. While EE46 and Cesdtot4 are our interested variables. So I don't remove all of them (Include the Totmmse4, if you think it can be removed, please let me know.). 
```{r}
library(mice)
miss.nu<-mice(df1, maxit=0)
md.pattern(df1,rotate.names=TRUE) 
fluxplot(df1)

#impt<-mice(data,maxit=30, seed=20221113,print = FALSE)
#stripplot(impt, CESDTOT4~.imp, pch=8, cex=2)
#plot(impt)
```

## KNN imputation. Define k as 1 which can prevent the producing of unresonable values
```{r}
library(DMwR2)
df_knn_step1<-knnImputation(df1[,c(-7,-8,-10,-18,-19)],k=1)

Totmmse4<-knnImputation(cbind(df_knn_step1,df1$Totmmse4),k=1)[,15]
Cesdtot4<-knnImputation(cbind(df_knn_step1,df1$Cesdtot4),k=1)[,15]
EE46<-knnImputation(cbind(df_knn_step1,df1$EE46),k=1)[,15]
HHA4<-knnImputation(cbind(df_knn_step1,df1$HHA4),k=1)[,15]
# CC43

df_knn<-cbind(df_knn_step1,Totmmse4,EE46,HHA4,Cesdtot4)
Totiadl<-df_knn$Totiadl4
```

###############################################################################################
#                           Muticollinearity of Totadl4                                       #
###############################################################################################
## Whether the Totadl4 should be discarded
# The VIF value of this feature indicate a weak muticollinearity. So we cannot remove it 
```{r}
## Based on the KNN imputation method
# VIF shows there is no need to discard Totadl4(VIF<5)

l1<-lm(Cesdtot4~.,df_knn)

car::vif(l1)
#write.csv(df_knn,'D:/BIOSTAT 650/Group project/Cleaned data.csv')
```

###############################################################################################
#           Whether the grouping of Totadl4 and Totiadl4 can be further simplified            #
###############################################################################################

## Whether the Totiadl4 can be simplified to three groups - (0,1,2), (3,4,5,6), (7,8,9,10)
```{r}
ggbetweenstats(
    data = df_knn,
    x = Totiadl4,
    y = Cesdtot4,
    type = "robust",
    pairwise.comparisons = F,
    outlier.tagging = TRUE
)
```

## Whether the Totadl4 can be simplified to two groups - 0, (1,7)
```{r}
ggbetweenstats(
    data = df_knn,
    x = Totadl4,
    y = Cesdtot4,
    type = "robust",
    pairwise.comparisons = F,
    outlier.tagging = TRUE
)
```

## GLH hypothesis of Totiadl4
```{r}
T0<-as.numeric(df_knn$Totiadl4==0)
T1<-as.numeric(df_knn$Totiadl4==1)
T2<-as.numeric(df_knn$Totiadl4==2)
T3<-as.numeric(df_knn$Totiadl4==3)
T4<-as.numeric(df_knn$Totiadl4==4)
T5<-as.numeric(df_knn$Totiadl4==5)
T6<-as.numeric(df_knn$Totiadl4==6)
T7<-as.numeric(df_knn$Totiadl4==7)
T8<-as.numeric(df_knn$Totiadl4==8)
T9<-as.numeric(df_knn$Totiadl4==9)
T10<-as.numeric(df_knn$Totiadl4==10)

## Taking T0 as reference coding
df1<-cbind(df_knn,T1,T2,T3,T4,
           T5,T6,T7,T8,T9,T10)
df2<-df1[,c(18:28)]
l2<-lm(Cesdtot4 ~.,df2)
summary(l2)
```

## Totiadl4 should only be combined to three groups
# p=8.995e-07 ***
```{r}
TM<-c(rep(0,1),1,rep(0,9),rep(0,1),1,-1,rep(0,8),rep(0,3),1,-1,rep(0,6),rep(0,4),1,-1,rep(0,5),rep(0,5),1,-1,rep(0,4),rep(0,7),1,-1,rep(0,2),rep(0,8),1,-1,rep(0,1),rep(0,9),1,-1)
tm<-matrix(TM,byrow = T,ncol=11)
print(tm)

car::linearHypothesis(model=l2,hypothesis.matrix=tm,rhs=rep(0,8))
```

## GLH hypothesis of Totadl4
```{r}
T0<-as.numeric(df_knn$Totadl4==0)
T1<-as.numeric(df_knn$Totadl4==1)
T2<-as.numeric(df_knn$Totadl4==2)
T3<-as.numeric(df_knn$Totadl4==3)
T4<-as.numeric(df_knn$Totadl4==4)
T5<-as.numeric(df_knn$Totadl4==5)
T6<-as.numeric(df_knn$Totadl4==6)
T7<-as.numeric(df_knn$Totadl4==7)


## Taking T0 as reference coding
df1<-cbind(df_knn,T1,T2,T3,T4,
           T5,T6,T7)
df2<-df1[,c(18:25)]
l2<-lm(Cesdtot4 ~.,df2)
summary(l2)
```

## Totadl4 cannot be further simplified 
# p=0.6192
```{r}
TM<-c(rep(0,1),1,-1,rep(0,5),rep(0,2),1,-1,rep(0,4),rep(0,3),1,-1,rep(0,3),rep(0,4),1,-1,rep(0,2),rep(0,5),1,-1,rep(0,1),rep(0,6),1,-1)
tm<-matrix(TM,byrow = T,ncol=8)
print(tm)

car::linearHypothesis(model=l2,hypothesis.matrix=tm,rhs=rep(0,6))
```

###############################################################################################
#              Whether the grouping of other variable can be further simplified               #
###############################################################################################
## Reglious
```{r}
ggbetweenstats(
    data = df_knn,
    x = EE46,
    y = Cesdtot4,
    type = "robust",
    pairwise.comparisons = F,
    outlier.tagging = TRUE
)
```

## Whether the (1,4) and (2,3) can be combined
```{r}
R1<-as.numeric(df_knn$EE46==1)
R2<-as.numeric(df_knn$EE46==2)
R3<-as.numeric(df_knn$EE46==3)
R4<-as.numeric(df_knn$EE46==4)

Cesdtot4=df_knn$Cesdtot4
df3<-cbind(Cesdtot4,R2,R3,R4)
df3<-as.data.frame(df3)
l3<-lm(Cesdtot4 ~.,df3)
summary(l3)
```

## Reglious cannot be combined to two groups
# p=0.5694
```{r}
TM1<-c(rep(0,1),1,-1,0,rep(0,3),1)
tm1<-matrix(TM1,byrow = T,ncol=4)

car::linearHypothesis(model=l3,hypothesis.matrix=tm1,rhs=rep(0,2))
```

## Reglious can be combined to two groups (1,2) and (3,4) - Because the group4 is too small, maybe this method is more suitable
# p=0.0506 .
```{r}
TM1<-c(rep(0,1),1,0,0,rep(0,2),1,-1)
tm1<-matrix(TM1,byrow = T,ncol=4)

car::linearHypothesis(model=l3,hypothesis.matrix=tm1,rhs=rep(0,2))
```

###############################################################################################
#                           Continuous vs categorical                                         #
###############################################################################################

## Before analysis, change Totiadl4 to three groups (0,1,2), (3,4,5,6), (7,8,9,10)
```{r}
## After this transfermation, it will be hard to say whether the variable is equal space. 
# So here I using reference coding to categorize the Totiadl4, using 1 as reference. 
Totiadl4<-df_knn$Totiadl4
Totiadl4[Totiadl4>=0 & Totiadl4<=2] = 1
Totiadl4[Totiadl4>=3 & Totiadl4<=6] = 2
Totiadl4[Totiadl4>=7 & Totiadl4<=10] = 3

df_knn$Totiadl4<-Totiadl4
dummyTotiadl4 <- class.ind(Totiadl4)

df_knn<-df_knn[,-7]  # remove the original Totiadl4
df_knn<-cbind(dummyTotiadl4[,2:3],df_knn)
colnames(df_knn)[1:2]<-c('Totiadl4_2','Totiadl4_3')
```

## Code EE46 as dummy variable
```{r}
EE46<-df_knn$EE46
EE46[EE46==1 | EE46==2] = 0
EE46[EE46==3 | EE46==4] = 1

df_knn$EE46<-EE46
```

## Before analysis, change Totiadl4 to three groups (0,1,2), (3,4,5,6), (7,8,9,10)
```{r}
l_Totiadl4<-lm(Cesdtot4~.,df_knn)
summary(l_Totiadl4)
```

## 
```{r}
Totiadl1<-median(Totiadl[Totiadl>=0 & Totiadl<=2])
Totiadl2<-median(Totiadl[Totiadl>=3 & Totiadl<=6])
Totiadl3<-median(Totiadl[Totiadl>=7 & Totiadl<=10])

Totiadl_median<-c(Totiadl1,Totiadl2,Totiadl3)
Totiadl_beta<-c(0,2.18878,2.40284)
plot(Totiadl_median,Totiadl_beta,type='o')
```

## Binary variable has no need to be dummy. Here I define the variable which has 3 - 5classes as muticlass. For this kinds of variables, we can use categorical methods. Other variable with more than 5 classes are defined as continuous. 
```{r}
type<-c()
for(i in 1:length(df_knn)){
  feature<-df_knn[,i]
  freq<-data.frame(table(feature))
  if(length(freq[,1]) == 2){
    type<-c(type,'binary')
  }else if(length(freq[,1]) >=3 & length(freq[,1])<=5){
    type<-c(type,'muticlass(3-5)')
  }else if(length(freq[,1]) > 5){
    type<-c(type,'continuous(>5)')
  }
}
data_type<-data.frame(type,colnames(df_knn))
print(data_type)
```

## Define the EE46 and Marstat as dummy variable. Both are taking group1 as reference group
```{r}
## Only EE46 and Marstat are defined as muticlasses variables
dummyMarstat <- class.ind(df_knn$Marstat)
dummyHealth <- class.ind(df_knn$HEALTH4)

df_knn<-df_knn[,c(-14,-6)]  # remove the original Totiadl4
df_knn<-cbind(dummyMarstat[,2:5],dummyHealth[,2:4],df_knn)
colnames(df_knn)[1:7]<-c('Marstat_2','Marstat_3','Marstat_4','Marstat_5',
                         'Health4_2','Health4_3','Health4_4')
```

###############################################################################################
#                            Covariate selection                                              #
###############################################################################################

## See selection notes above (based on literature)

```{r}
colnames(df_knn)
detach( "package:MASS", unload = TRUE ) # I want 'select' from dplyr
df_knn2 <- df_knn %>% select(-c(NKIDS4, HHA4, NFRAC41))
fullmodel <- lm(Cesdtot4 ~. , data=df_knn2)
summary(fullmodel)

# From here, we can either do a partial F test to test if we can remove some variables, or just go straight into the meditation/ interaction tests.
```


## Stepwise selection (or sequential replacement), which is a combination of forward and backward selections. You start with no predictors, then sequentially add the most contributive predictors (like forward selection). After adding each new variable, remove any variables that no longer provide an improvement in the model fit (like backward selection).

```{r}
# Fit the full model 
full.model <- lm(Cesdtot4 ~., data = df_knn) 


# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = FALSE)
summary(step.model)
```

```{r}
## No wrong for lack of fit
anova(full.model,step.model)
```

## Remove the feature and obtain the new dataset named Reduced_df_knn
# Create the final full model - 17 variables
```{r}
Reduced_df_knn<-step.model$model
```


###############################################################################################
#                                    Interaction                                              #
###############################################################################################
## U43s has interaction with Khyper41? 
# The interaction U43s:Khyper41 is not significant
```{r}
## Taken Cesdtot4 as response
l3<-lm(Cesdtot4 ~ Marstat_2 + Marstat_3 + Marstat_4 + Health4_3 + Health4_4 + 
    Totiadl4_2 + Totiadl4_3 + Grade + USborn + OO49lang + 
    Sex + Totadl4 + Totmmse4 + EE46 + U43s*Khyper41, df_knn)

summary(l3)

## EE46 - Sex
## 
```

## Although the interaction is not significant. We still can test whether there is one mediation relationship between them. 
# The mediation is complete mediation. U43s influence the Cesdtot4 variable completely through the Khyper41.
# Should not include both U43s and Khyper41 in our final model. While keep which one in the model is one question. Because we need to point out which is confounder: https://www.scribbr.com/methodology/confounding-variables/

# https://www.sohu.com/a/429507752_120233365
# http://www.davidakenny.net/cm/mediate.html
# https://ademos.people.uic.edu/Chapter14.html

# p=2.113565e-06, may have a strong mediation. 
```{r}
## My result indicate that U43s may have one directed effect on Cesdtot4. While Khyper41 may be the mediation. 
library(multilevel)
sobel(pred=df_knn$U43s, med=df_knn$Khyper41, out=df_knn$Cesdtot4)

## sobel test only give out the z value, so here I just compute p-value by hand

print(2-2*pnorm(4.752171)) 
#sobel(pred=Reduced_df_knn$Khyper41, med=Reduced_df_knn$U43s, out=Reduced_df_knn$Cesdtot4)
```

## Totiadl4_3 and U43s interaction 
# The interaction EE46 is not significant
```{r}
## Taken Cesdtot4 as response
l3<-lm(Cesdtot4 ~ Marstat_2 + Marstat_3 + Marstat_4 + Health4_3 + Health4_4 + 
    Totiadl4_2 + Totiadl4_3 + Grade + USborn + OO49lang + 
    Sex + Totadl4 + Totmmse4 + EE46 + Totiadl4_3*U43s , df_knn)

summary(l3)
```

```{r}
## My result indicate that U43s may have one directed effect on Cesdtot4. While Khyper41 may be the mediation. 
library(multilevel)
sobel(pred=df_knn$Totiadl4_3, med=df_knn$U43s, out=df_knn$Cesdtot4)

## sobel test only give out the z value, so here I just compute p-value by hand

print(2-2*pnorm(2.27254)) 
#sobel(pred=Reduced_df_knn$Khyper41, med=Reduced_df_knn$U43s, out=Reduced_df_knn$Cesdtot4)
```


```{r}
#Reduced_df_knn<-df_knn[,c('EE46_2','EE46_3','Marstat_2','Marstat_3','Marstat_4',
#    'Health4_3','Health4_4','Totiadl4_2','Totiadl4_3','Grade',
#    'USborn','U43s','OO49lang','Sex','Totadl4','Totmmse4','Cesdtot4')]
U43s = df_knn$U43s
Reduced_df_knn<-cbind(Reduced_df_knn[,-11],U43s)
```

###############################################################################################
#                                   Diagnostics                                               #
###############################################################################################
# R for solve the heter Heteroskedasticity. https://rpubs.com/cyobero/187387
```{r}
## Define the final full model
l4<-lm(Cesdtot4 ~Marstat_2 + Marstat_3 + Marstat_4 + Health4_3 + Health4_4 + 
    Totiadl4_2 + Totiadl4_3 + Grade + USborn + OO49lang + 
    Sex + Totadl4 + Totmmse4 + EE46 + Totiadl4_3*U43s,Reduced_df_knn)

summary(l4)

par(mfrow=c(2,2))
plot(l4)
```

## Although the normality test is violated, the residual's distribution is skewed and has only one peak. In my experience, this situation is doesn't matter. 
```{r}
## Normality test for the residuals
shapiro.test(l4$residuals) # Normality is violated - p-value<0.001

hist(l4$residuals) ## Skewed distribution but only one peak
```

## Durbin-Watson Test for autocorrelation - significant
# The plot seems to have cluster pattern and the test is also a little bit significant
# p-value = 0.05093 - I didn't take any method to deal with this problem. Because the problem is not very serious. 
```{r}
plot(l4$res[-1682],l4$res[-1],xlab='residual_{i}',ylab='residual_{i-1}')
dwtest(l4)
```

## Constant Variance Illustration
# Robust Standard Errors - Solving Heteroskedasticity
# Strange pattern -  p-value = 2.189e-09
```{r}
plot(l4$fitted.values,l4$residuals,xlab='Fitted_value',ylab='Residuals')
bptest(l4)
coeftest(l4, vcov = vcovHC(l4, "HC1"))
```


###############################################################################################
#                                      Outliers                                               #
###############################################################################################

## Leverage
```{r}
Leverage<-hatvalues(l4)
Average_leverage<-mean(Leverage)
type = Leverage

length(Leverage[Leverage>2*Average_leverage])

type[Leverage>2*Average_leverage] = 'High leverage'
type[Leverage<=2*Average_leverage] = 'Normal'

dleverage<-data.frame(Leverage,x=1:length(Reduced_df_knn[,1]),type=type)
ggplot(dleverage,aes(x=x,y=Leverage))+
  geom_point(aes(col=type))+geom_hline(aes(yintercept=2*Average_leverage,colour="Cutoff")) +
  geom_text(aes(0,2*Average_leverage,label = round(2*Average_leverage,4), vjust = -1))
```

## Cook_Distance
```{r}
external_residuals<-rstudent(l4)
Cook_Distance<-(rstandard(l4)**2/17)*(Leverage/(1-Leverage))
type = Cook_Distance

length(Cook_Distance[Cook_Distance>4/length(Reduced_df_knn[,1])])

type[Cook_Distance>4/length(Reduced_df_knn[,1])] = 'High influence'
type[Cook_Distance<=4/length(Reduced_df_knn[,1])] = 'Normal'

dcook<-data.frame(Cook_Distance,x=1:length(Reduced_df_knn[,1]),type)

ggplot(dcook,aes(x=x,y=Cook_Distance))+
  geom_point(aes(colour=type))+geom_hline(aes(yintercept=4/length(Reduced_df_knn[,1]),colour="Cutoff")) +
  geom_text(aes(0,4/length(Reduced_df_knn[,1]),label = round(4/length(Reduced_df_knn[,1]),4), vjust = -1))
```

## None of the high leverage points or high influence points are come from the missing value part - don't create the outliers
# High proportion of outliers making it impossible to remove all of them. - Using robust regression. 
# Later I will compare the l5 model (remove the sample both in high leverage and influence sets) with the robust regression to double check the significant. 
```{r}
dinfluence<-data.frame(
  leverage = dleverage$type,
  cook = dcook$type,
  NA_Count
)

dinfluence[dinfluence$leverage=='High leverage' & dinfluence$cook == 'High influence',]
dinfluence[dinfluence$NA_Count>0,]
length(dinfluence[dinfluence$leverage=='High leverage' & dinfluence$cook == 'High influence',1])
```

## Due to the abnormal pattern in residuals and high proportion of influence points, I tried several methods below. However, none of them can completely solved the problems. You can still try some other methods. If you have opinions, please let me know. 

## M-estimation
# Huber loss
```{r}
library('sfsmisc')
robust_regression_huber<-rlm(Cesdtot4 ~ Marstat_2 + Marstat_3 + Marstat_4 + Health4_3 + Health4_4 + 
    Totiadl4_2 + Totiadl4_3 + Grade + USborn + OO49lang + 
    Sex + Totadl4 + Totmmse4 + EE46 + Totiadl4_3*U43s,Reduced_df_knn)

F_statistics<-c()
p_value<-c()

feature<-rownames(data.frame(robust_regression_huber$coefficients))

for(i in 1:length(feature)){
  ftest<-f.robftest(robust_regression_huber,var=feature[i])
  F_statistics<-c(F_statistics,ftest$statistic)
  p_value<-c(p_value,ftest$p.value)
}

significant<-p_value
significant[p_value<=0.001] = '***'
significant[p_value<=0.01 & p_value>0.001] = '**'
significant[p_value<=0.05 & p_value>0.01] = '*'
significant[p_value<=0.1 & p_value>0.05] = '.'
significant[p_value<=1 & p_value>0.1] = ' '

robust_result<-data.frame(Robust_Estimation = robust_regression_huber$coefficients)
robust_result<-cbind(robust_result,F_statistics,p_value,significant)
robust_result
```

###############################################################################################
#                                   Residual's plot                                           #
###############################################################################################

## 
```{r}
d<-data.frame(residuals=l4$residuals,fittedy = l4$fitted.values,y = Reduced_df_knn$Cesdtot4)
ggplot(d,aes(residuals,fittedy,col=factor(y)))+geom_point()

```

## Comparing the result from robust regression, least square(Remove the outliers), robust standard error
```{r}
robust_result # Robust - huber loss
summary(l4) # OLS
coeftest(l4, vcov = vcovHC(l4, "HC1")) ## Robust SE
```





